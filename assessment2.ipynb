{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import gymnasium.logger\n",
    "import gymnasium.wrappers.record_video\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.layers\n",
    "import uuid\n",
    "import typing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(name, video: bool = True):\n",
    "    folder_name = f\"./video/{name}/{uuid.uuid4()}\"\n",
    "    env = gymnasium.make(name, render_mode=\"rgb_array\")\n",
    "    if video:\n",
    "        env = gymnasium.wrappers.record_video.RecordVideo(env, folder_name, disable_logger=True)\n",
    "        return env, folder_name\n",
    "    else:\n",
    "        return env, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    def __init__(self, action_space: int, state_space: int, learning_rate: float = 0.3, discount_factor: float = 0.7):\n",
    "        self.q = np.random.random((state_space, action_space))\n",
    "        self.alpha = learning_rate\n",
    "        self.gamma = discount_factor\n",
    "        \n",
    "    def update(\n",
    "        self, states: typing.List[int], next_states: typing.List[int],\n",
    "        actions_taken: typing.List[int], rewards: typing.List[float]\n",
    "    ):\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            next_state = next_states[i]\n",
    "            action_taken = actions_taken[i]\n",
    "            reward = rewards[i]\n",
    "            \n",
    "            self.q[state][action_taken] = self.q[states][action_taken] + self.alpha * (\n",
    "                reward + self.gamma * self.max_reward(next_states) - self.q[state][action_taken]\n",
    "            )\n",
    "\n",
    "    def max_reward(self, state: int) -> float:\n",
    "        return max(self.qt(state))\n",
    "\n",
    "    def qt(self, state: int) -> typing.List[float]:\n",
    "        return list(self.q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNeural:\n",
    "    def __init__(self, action_space: int, state_space: int, discount_factor: float = 0.7):\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.gamma = discount_factor\n",
    "\n",
    "        self.model = tensorflow.keras.Sequential([\n",
    "            tensorflow.keras.layers.Input(shape=(state_space,)),\n",
    "            tensorflow.keras.layers.Dense(512, activation=\"relu\"),\n",
    "            tensorflow.keras.layers.Dense(action_space, activation=\"linear\"),\n",
    "        ])\n",
    "        self.model.compile(loss='mse', optimizer=tensorflow.keras.optimizers.legacy.Adam())\n",
    "\n",
    "    def update(\n",
    "        self, states: typing.List[int], next_states: typing.List[int],\n",
    "        actions_taken: typing.List[int], rewards: typing.List[float]\n",
    "    ):\n",
    "        state_one_hot = tensorflow.one_hot(states, self.state_space)\n",
    "        state_next_one_hot = tensorflow.one_hot(next_states, self.state_space)\n",
    "        masks = tensorflow.one_hot(actions_taken, self.action_space)\n",
    "\n",
    "        target = self.model.predict(state_next_one_hot, verbose=0)\n",
    "        for i in range(len(rewards)):\n",
    "            r = masks[i] * rewards[i]\n",
    "            target[i] += (self.gamma * r)\n",
    "        \n",
    "        self.model.fit(state_one_hot, target, epochs=1, verbose=0)\n",
    "        \n",
    "    def qt(self, state: int) -> typing.List[float]:\n",
    "        state_tensor = tensorflow.one_hot(state, self.state_space)\n",
    "        state_tensor = tensorflow.expand_dims(state_tensor, 0)\n",
    "        action_probs = self.model.predict(state_tensor, verbose=0)\n",
    "        action = tensorflow.argmax(action_probs[0]).numpy()\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, action_space: int, state_space: int, epsilon_decay: float = 0.99):\n",
    "        self.epsilon = 1\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "\n",
    "    def next_action(self, q: typing.List[typing.Tuple[int, float]], state: int) -> int:\n",
    "        n = np.random.rand()\n",
    "        if n < self.epsilon:\n",
    "            action = np.random.randint(0, self.action_space)\n",
    "        else:\n",
    "            action = max(q, key=lambda a: a[1])[0]\n",
    "            \n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound:\n",
    "    def __init__(self, action_space: int, state_space: int, p_decay: float = 4.0):\n",
    "        self.p_decay = p_decay\n",
    "        self.t = 1\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.count_selected = np.ones((state_space, action_space))\n",
    "\n",
    "    def next_action(self, q: typing.List[typing.Tuple[int, float]], state: int) -> int:\n",
    "        ut = np.sqrt(\n",
    "            (self.p_decay * np.log(self.t)) / np.array(self.count_selected[state])\n",
    "        )\n",
    "        qt = np.array(q) + ut\n",
    "        \n",
    "        action = max(enumerate(qt), key=lambda n: n[1])[0]\n",
    "\n",
    "        self.count_selected[state][action] += 1\n",
    "        self.t += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(env, num_episodes: int, selection_method, learning_method):\n",
    "    batch_size = 64\n",
    "    update_after_actions = 32\n",
    "    max_steps_per_episode = 1000\n",
    "    \n",
    "    l = learning_method(env.action_space.n, env.observation_space.n)\n",
    "    s = selection_method(env.action_space.n, env.observation_space.n)\n",
    "\n",
    "    episode_mean_reward = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        round = 0\n",
    "        rewards_history = []\n",
    "        action_history = []\n",
    "        state_history = []\n",
    "        state_next_history = []\n",
    "\n",
    "        if episode % 20 == 0:\n",
    "            print(f\"Episode {episode}...\")\n",
    "        observation, state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done and round < max_steps_per_episode:\n",
    "            round += 1\n",
    "            \n",
    "            action = s.next_action(l.qt(observation), observation)\n",
    "\n",
    "            old_observation = observation\n",
    "            observation, reward, done, _, state = env.step(action)\n",
    "\n",
    "            rewards_history.append(reward)\n",
    "            action_history.append(action)\n",
    "            state_history.append(old_observation)\n",
    "            state_next_history.append(observation)\n",
    "\n",
    "            if round % update_after_actions == 0 and len(rewards_history) > batch_size:\n",
    "                indices = list(range(len(rewards_history) - batch_size, len(rewards_history)))\n",
    "                \n",
    "                state_sample = [state_history[i] for i in indices]\n",
    "                state_next_sample = [state_next_history[i] for i in indices]\n",
    "                action_sample = [action_history[i] for i in indices]\n",
    "                rewards_sample = [rewards_history[i] for i in indices]\n",
    "            \n",
    "                l.update(state_sample, state_next_sample, action_sample, rewards_sample)\n",
    "\n",
    "        episode_mean_reward.append(np.mean(rewards_history))\n",
    "\n",
    "    env.close()\n",
    "    return episode_mean_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0...\n"
     ]
    }
   ],
   "source": [
    "env, folder_name = create_environment(\"Taxi-v3\", False)\n",
    "rewards = learn(env, 100, UpperConfidenceBound, QNeural)\n",
    "\n",
    "print(f\"Video in {folder_name}\")\n",
    "\n",
    "window = 20\n",
    "average_reward = []\n",
    "for _ in range(window - 1):\n",
    "    average_reward.insert(0, np.nan)\n",
    "for x in range(len(rewards) - window + 1):\n",
    "     average_reward.append(np.mean(rewards[x:x+window]))\n",
    "\n",
    "plt.plot(rewards, label=\"Reward\")\n",
    "plt.plot(average_reward, label=\"Rolling mean reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
