{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "import gymnasium.logger\n",
    "import gymnasium.wrappers.record_video\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.keras\n",
    "import tensorflow.keras.layers\n",
    "import uuid\n",
    "import typing\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment(name, video: bool = True):\n",
    "    \"\"\"\n",
    "    Create an OpenAI Gym environment with optional video recording.\n",
    "\n",
    "    Parameters:\n",
    "    - name (str): The name of the OpenAI Gym environment.\n",
    "    - video (bool): Flag indicating whether to record videos (default is True).\n",
    "\n",
    "    Returns:\n",
    "    - env (gym.Env): The created OpenAI Gym environment.\n",
    "    - folder_name (str or None): The folder path where video recordings will be saved if video is enabled,\n",
    "                                 otherwise, None.\n",
    "    \"\"\"\n",
    "    # Generate a unique folder name for video recordings\n",
    "    folder_name = f\"./video/{name}/{uuid.uuid4()}\"\n",
    "    \n",
    "    # Create the OpenAI Gym environment with RGB rendering\n",
    "    env = gymnasium.make(name, render_mode=\"rgb_array\")\n",
    "\n",
    "    if video:\n",
    "        # Wrap the environment to record video with specified folder and disable logger\n",
    "        env = gymnasium.wrappers.record_video.RecordVideo(env, folder_name, disable_logger=True)\n",
    "        return env, folder_name\n",
    "    else:\n",
    "        return env, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Table implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"\n",
    "    A simple Q-table-based reinforcement learning agent.\n",
    "\n",
    "    Parameters:\n",
    "    - action_space (int): The number of possible actions in the environment.\n",
    "    - state_space (int): The number of possible states in the environment.\n",
    "    - learning_rate (float): The learning rate for updating Q-values (default is 0.3).\n",
    "    - discount_factor (float): The discount factor for future rewards in Q-value updates (default is 0.7).\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space: int, state_space: int, learning_rate: float = 0.3, discount_factor: float = 0.7):\n",
    "        # Initialize the Q-table with random values\n",
    "        self.q = np.random.random((state_space, action_space))\n",
    "        self.alpha = learning_rate  # Learning rate for Q-value updates\n",
    "        self.gamma = discount_factor  # Discount factor for future rewards\n",
    "\n",
    "    def update(\n",
    "        self, states: typing.List[int], next_states: typing.List[int],\n",
    "        actions_taken: typing.List[int], rewards: typing.List[float]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update Q-values based on experienced transitions.\n",
    "\n",
    "        Parameters:\n",
    "        - states (List[int]): List of current states.\n",
    "        - next_states (List[int]): List of next states.\n",
    "        - actions_taken (List[int]): List of actions taken.\n",
    "        - rewards (List[float]): List of rewards received.\n",
    "        \"\"\"\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            next_state = next_states[i]\n",
    "            action_taken = actions_taken[i]\n",
    "            reward = rewards[i]\n",
    "\n",
    "            # Q-value update using the Q-learning formula\n",
    "            self.q[state][action_taken] = self.q[state][action_taken] + self.alpha * (\n",
    "                reward + self.gamma * self.max_reward(next_state) - self.q[state][action_taken]\n",
    "            )\n",
    "\n",
    "    def max_reward(self, state: int) -> float:\n",
    "        \"\"\"\n",
    "        Get the maximum Q-value for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int): The state for which the maximum Q-value is to be obtained.\n",
    "\n",
    "        Returns:\n",
    "        - float: The maximum Q-value for the specified state.\n",
    "        \"\"\"\n",
    "        return max(self.qt(state))\n",
    "\n",
    "    def qt(self, state: int) -> typing.List[float]:\n",
    "        \"\"\"\n",
    "        Get the list of Q-values for all actions in a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int): The state for which Q-values are to be obtained.\n",
    "\n",
    "        Returns:\n",
    "        - List[float]: The list of Q-values for all actions in the specified state.\n",
    "        \"\"\"\n",
    "        return list(self.q[state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import typing\n",
    "import numpy as np\n",
    "\n",
    "class QNeural:\n",
    "    \"\"\"\n",
    "    A neural network-based Q-learning agent.\n",
    "\n",
    "    Parameters:\n",
    "    - action_space (int): The number of possible actions in the environment.\n",
    "    - state_space (int): The number of possible states in the environment.\n",
    "    - discount_factor (float): The discount factor for future rewards in Q-value updates (default is 0.7).\n",
    "    - learning_factor (float): The learning factor for the neural network (default is 0.01).\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space: int, state_space: int, discount_factor: float = 0.7, learning_factor: float = 0.05):\n",
    "\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.eps = 1.0\n",
    "        self.gamma = discount_factor\n",
    "        self.learning_factor = learning_factor\n",
    "\n",
    "        # Define a neural network model using Keras\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Input(shape=(state_space,), dtype=tf.float32),\n",
    "            tf.keras.layers.Dense(8, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(16, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(32, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(256, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(128, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(64, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(32, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(16, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(8, activation=\"linear\"),\n",
    "            tf.keras.layers.Dense(action_space, activation=\"linear\")\n",
    "        ])\n",
    "\n",
    "        # Example with Adam optimizer and a different learning rate\n",
    "        self.model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=learning_factor))\n",
    "\n",
    "    def update(\n",
    "        self, states: typing.List[int], next_states: typing.List[int],\n",
    "        actions_taken: typing.List[int], rewards: typing.List[float]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Update the neural network's weights based on experienced transitions.\n",
    "\n",
    "        Parameters:\n",
    "        - states (List[int]): List of current states.\n",
    "        - next_states (List[int]): List of next states.\n",
    "        - actions_taken (List[int]): List of actions taken.\n",
    "        - rewards (List[float]): List of rewards received.\n",
    "        \"\"\"\n",
    "        state_one_hot = tf.one_hot(states, self.state_space, dtype=tf.float32)\n",
    "        state_next_one_hot = tf.one_hot(next_states, self.state_space, dtype=tf.float32)\n",
    "        masks = tf.one_hot(actions_taken, self.action_space, dtype=tf.float32)\n",
    "\n",
    "        # Predict Q-values for the current states and next states\n",
    "        current_q_values = self.model.predict(state_one_hot, verbose=0)\n",
    "        next_q_values = self.model.predict(state_next_one_hot, verbose=0)\n",
    "\n",
    "        # Calculate the target Q-values using the Q-learning update rule\n",
    "        targets = current_q_values.copy()\n",
    "        for i in range(len(rewards)):\n",
    "            targets[i, actions_taken[i]] = rewards[i] + self.gamma * np.max(next_q_values[i])\n",
    "\n",
    "        # Update the model's weights using the modified Q-values\n",
    "        self.model.fit(state_one_hot, targets, epochs=1, verbose=0)\n",
    "\n",
    "    def qt(self, state: int) -> int:\n",
    "        \"\"\"\n",
    "        Get the action with the highest Q-value for a given state.\n",
    "\n",
    "        Parameters:\n",
    "        - state (int): The state for which the action with the highest Q-value is to be obtained.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        state_tensor = tf.one_hot(state, self.state_space, dtype=tf.float32)\n",
    "        state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "        action_probs = self.model(state_tensor, training=False)\n",
    "        action = np.argmax(action_probs[0])\n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    \"\"\"\n",
    "    An epsilon-greedy exploration strategy for action selection.\n",
    "\n",
    "    Parameters:\n",
    "    - action_space (int): The number of possible actions in the environment.\n",
    "    - state_space (int): The number of possible states in the environment.\n",
    "    - epsilon_decay (float): The rate at which the exploration probability epsilon decays (default is 0.99).\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space: int, state_space: int, epsilon_decay: float = 0.99):\n",
    "        self.epsilon = 1  # Initial exploration probability\n",
    "        self.epsilon_decay = epsilon_decay  # Rate at which epsilon decays\n",
    "        self.action_space = action_space  # Number of possible actions\n",
    "        self.state_space = state_space  # Number of possible states\n",
    "\n",
    "    def next_action(self, q: typing.List[typing.Tuple[int, float]], state: int) -> int:\n",
    "        \"\"\"\n",
    "        Select the next action using epsilon-greedy strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - q (List[Tuple[int, float]]): List of tuples containing actions and their associated Q-values.\n",
    "        - state (int): The current state.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        n = np.random.rand()  # Random number from a uniform distribution [0, 1)\n",
    "\n",
    "        if n < self.epsilon:\n",
    "            action = np.random.randint(0, self.action_space)  # Random exploration\n",
    "        else:\n",
    "            action = np.argmax(q)  # Exploitation, choose action with highest Q-value\n",
    "            \n",
    "        if self.epsilon > 0.01:\n",
    "            self.epsilon *= self.epsilon_decay  # Decay epsilon over time\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpperConfidenceBound:\n",
    "    \"\"\"\n",
    "    An Upper Confidence Bound (UCB) exploration strategy for action selection.\n",
    "\n",
    "    Parameters:\n",
    "    - action_space (int): The number of possible actions in the environment.\n",
    "    - state_space (int): The number of possible states in the environment.\n",
    "    - p_decay (float): The parameter controlling the exploration-exploitation balance (default is 4.0).\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space: int, state_space: int, p_decay: float = 1.0):\n",
    "        self.p_decay = p_decay  # Parameter controlling exploration-exploitation balance\n",
    "        self.t = 1  # Time step\n",
    "        self.action_space = action_space  # Number of possible actions\n",
    "        self.state_space = state_space  # Number of possible states\n",
    "        self.count_selected = np.ones((state_space, action_space))  # Count of selections for each action\n",
    "\n",
    "    def next_action(self, q: typing.List[typing.Tuple[int, float]], state: int) -> int:\n",
    "        \"\"\"\n",
    "        Select the next action using the Upper Confidence Bound (UCB) strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - q (List[Tuple[int, float]]): List of tuples containing actions and their associated Q-values.\n",
    "        - state (int): The current state.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        ut = np.sqrt(\n",
    "            (self.p_decay * np.log(self.t)) / np.array(self.count_selected[state])\n",
    "        )\n",
    "        qt = np.array(q) + ut\n",
    "        \n",
    "        # Choose the action with the highest UCB-augmented Q-value\n",
    "        action = max(enumerate(qt), key=lambda n: n[1])[0]\n",
    "\n",
    "        # Update selection count and time step\n",
    "        self.count_selected[state][action] += 1\n",
    "        self.t += 1\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoltzmannExploration:\n",
    "    \n",
    "    \"\"\"\n",
    "    An agent using the Boltzmann exploration strategy for action selection.\n",
    "\n",
    "    Parameters:\n",
    "    - action_space (int): The number of possible actions in the environment.\n",
    "    - state_space (int): The number of possible states in the environment.\n",
    "    - temperature (float): The temperature parameter controlling exploration (default is 1.0).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, action_space: int, state_space: int, temperature: float = 0.1):\n",
    "        self.action_space = action_space\n",
    "        self.state_space = state_space\n",
    "        self.temperature = temperature\n",
    "\n",
    "    \n",
    "    def next_action(self, q: typing.List[typing.Tuple[int, float]], state: int) -> int:\n",
    "        \"\"\"\n",
    "        Select the next action using the Boltzmann exploration strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - q (List[Tuple[int, float]]): List of tuples containing actions and their associated Q-values.\n",
    "        - state (int): The current state.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        # Calculate the probabilities of each action using the Boltzmann formula\n",
    "        probabilities = np.exp(np.array([q_value / self.temperature for q_value in q]))\n",
    "        probabilities /= np.sum(probabilities)\n",
    "\n",
    "        # Choose an action based on the calculated probabilities\n",
    "        action = np.argmax(probabilities)\n",
    "\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling:\n",
    "    \"\"\"\n",
    "    A ThompsonSampling exploration strategy for action selection.\n",
    "    \n",
    "    Parameters:\n",
    "    - action_space (int): The number of possible actions in the environment.\n",
    "    - state_space (int): The number of possible states in the environment.\n",
    "    \"\"\"\n",
    "    def __init__(self, action_space: int, state_space: int):\n",
    "        self.action_space = action_space  # Number of possible actions\n",
    "        self.state_space = state_space  # Number of possible states\n",
    "        self.std_dev = np.ones((state_space, action_space))  # Standard deviation for each action's Q-value\n",
    "\n",
    "    def next_action(self, q: typing.List[typing.Tuple[int, float]], state: int) -> int:\n",
    "        \"\"\"\n",
    "        Select the next action using the Thompson Sampling strategy.\n",
    "\n",
    "        Parameters:\n",
    "        - q (List[Tuple[int, float]]): List of tuples containing actions and their associated Q-values.\n",
    "        - state (int): The current state.\n",
    "\n",
    "        Returns:\n",
    "        - int: The selected action.\n",
    "        \"\"\"\n",
    "        sampled_q_values = np.random.normal(q, self.std_dev[state])\n",
    "        return np.argmax(sampled_q_values)\n",
    "\n",
    "    def update(self, states: typing.List[int], next_states: typing.List[int], actions_taken: typing.List[int], rewards: typing.List[float], q_table: typing.List[typing.Tuple[int, float]], discount_factor:float = 0.9):\n",
    "        \"\"\"\n",
    "        Calculate the Temporal Difference error and update the standard deviation.\n",
    "\n",
    "        Args:\n",
    "            state (int): current state\n",
    "            next_state (int): next state\n",
    "            action (int): current action\n",
    "            next_action (int): next action\n",
    "            reward (int): reward\n",
    "            q_table (typing.List[typing.Tuple[int, float]]): Q-table\n",
    "            discount_factor (float, optional): discount factor (defaults to 0.9)\n",
    "        \"\"\"\n",
    "        for i in range(len(states)):\n",
    "            state = states[i]\n",
    "            next_state = next_states[i]\n",
    "            action_taken = actions_taken[i]\n",
    "            reward = rewards[i]\n",
    "            \n",
    "            action = self.next_action(q_table[next_state], next_state)\n",
    "            \n",
    "            # Calculate the TD error\n",
    "            q_current = q_table[state, action_taken]  # Current Q-value\n",
    "            q_next = q_table[next_state, action]  # Next Q-value\n",
    "            td_error = reward + discount_factor * q_next - q_current  # TD Error\n",
    "\n",
    "            # Update the standard deviation\n",
    "            self.std_dev[state][action_taken] *= np.exp(-np.abs(td_error))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def learn(env, num_episodes: int, selection_method, learning_method, print_episodes: bool = False):\n",
    "    \"\"\"\n",
    "    Train a reinforcement learning agent using a specified selection method and learning method.\n",
    "\n",
    "    Parameters:\n",
    "    - env: The OpenAI Gym environment.\n",
    "    - num_episodes (int): The number of episodes to train the agent.\n",
    "    - selection_method: The exploration strategy for action selection.\n",
    "    - learning_method: The learning method used to update the agent's knowledge.\n",
    "    - print_episodes (bool): Flag indicating whether to print the episode number (default is False).\n",
    "\n",
    "    Returns:\n",
    "    - episode_mean_reward (List[float]): List containing the mean reward for each episode during training.\n",
    "    \"\"\"\n",
    "    batch_size = 20\n",
    "    max_steps_per_episode = 1000\n",
    "    episode_mean_reward = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        round = 0\n",
    "        rewards_history = []\n",
    "        action_history = []\n",
    "        state_history = []\n",
    "        state_next_history = []\n",
    "        consecutive_stuck = 0\n",
    "\n",
    "        if episode % 20 == 0 and print_episodes:\n",
    "            print(f\"Episode {episode}...\")\n",
    "\n",
    "        observation, state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done and round < max_steps_per_episode:\n",
    "            round += 1\n",
    "\n",
    "            # Select the next action using the specified selection method\n",
    "            action = selection_method.next_action(learning_method.qt(observation, state), observation)\n",
    "\n",
    "            old_observation = observation\n",
    "            observation, reward, done, _, state = env.step(action)\n",
    "\n",
    "            if isinstance(selection_method, ThompsonSampling):\n",
    "                selection_method.update(old_observation, observation, action,\n",
    "                                         selection_method.next_action(learning_method.qt(observation), observation),\n",
    "                                         reward, learning_method.q)\n",
    "\n",
    "            # Check if the Taxi is stuck in one place for more than 3 moves\n",
    "            if old_observation == observation:\n",
    "                consecutive_stuck += 1\n",
    "            else:\n",
    "                consecutive_stuck = 0\n",
    "\n",
    "            # Apply penalty if stuck for more than 3 moves\n",
    "            if consecutive_stuck > 3:\n",
    "                reward += -10\n",
    "\n",
    "            # Store the information for experience replay\n",
    "            rewards_history.append(reward)\n",
    "            action_history.append(action)\n",
    "            state_history.append(old_observation)\n",
    "            state_next_history.append(observation)\n",
    "\n",
    "            # Perform learning updates after a certain number of actions\n",
    "            if round % batch_size == 0:\n",
    "                indices = list(range(len(rewards_history) - batch_size, len(rewards_history)))\n",
    "\n",
    "                state_sample = [state_history[i] for i in indices]\n",
    "                state_next_sample = [state_next_history[i] for i in indices]\n",
    "                action_sample = [action_history[i] for i in indices]\n",
    "                rewards_sample = [rewards_history[i] for i in indices]\n",
    "\n",
    "                # Update the agent's knowledge using the specified learning method\n",
    "                learning_method.update(state_sample, state_next_sample, action_sample, rewards_sample)\n",
    "\n",
    "        episode_mean_reward.append(np.mean(rewards_history))\n",
    "\n",
    "    env.close()\n",
    "    return episode_mean_reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'state_space'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb Cell 15\u001b[0m line \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m env, folder_name \u001b[39m=\u001b[39m create_environment(\u001b[39m\"\u001b[39m\u001b[39mTaxi-v3\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Train the agent using Upper Confidence Bound for action selection and QNeural for learning\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m rewards \u001b[39m=\u001b[39m learn(env, \u001b[39m100\u001b[39;49m, EpsilonGreedy, QNeural, print_episodes \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Display information about video recording folder\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mVideo in \u001b[39m\u001b[39m{\u001b[39;00mfolder_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb Cell 15\u001b[0m line \u001b[0;36mlearn\u001b[0;34m(env, num_episodes, selection_method, learning_method, print_episodes)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39mround\u001b[39m \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Select the next action using the specified selection method\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m action \u001b[39m=\u001b[39m selection_method\u001b[39m.\u001b[39mnext_action(learning_method\u001b[39m.\u001b[39;49mqt(observation, state), observation)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m old_observation \u001b[39m=\u001b[39m observation\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m observation, reward, done, _, state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "\u001b[1;32m/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb Cell 15\u001b[0m line \u001b[0;36mQNeural.qt\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mqt\u001b[39m(\u001b[39mself\u001b[39m, state: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m \u001b[39m    Get the action with the highest Q-value for a given state.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m \u001b[39m    - int: The selected action.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m     state_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mone_hot(state, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate_space, dtype\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mfloat32)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     state_tensor \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mexpand_dims(state_tensor, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/markodin/Desktop/CS4049_ASS2/assessment2.ipynb#X20sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     action_probs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(state_tensor, training\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'state_space'"
     ]
    }
   ],
   "source": [
    "# Create the environment without video recording\n",
    "env, folder_name = create_environment(\"Taxi-v3\", True)\n",
    "\n",
    "# Train the agent using Upper Confidence Bound for action selection and QNeural for learning\n",
    "rewards = learn(env, 100, EpsilonGreedy, QNeural, print_episodes = True)\n",
    "\n",
    "# Display information about video recording folder\n",
    "print(f\"Video in {folder_name}\")\n",
    "\n",
    "# Calculate rolling mean of rewards for smoother visualization\n",
    "window = 20\n",
    "average_reward = []\n",
    "for _ in range(window - 1):\n",
    "    average_reward.insert(0, np.nan)\n",
    "for x in range(len(rewards) - window + 1):\n",
    "     average_reward.append(np.mean(rewards[x:x+window]))\n",
    "\n",
    "# Plot the rewards and rolling mean rewards\n",
    "plt.plot(rewards, label=\"Reward\")\n",
    "plt.plot(average_reward, label=\"Rolling mean reward\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train_and_plot(env, num_episodes, action_selections, learning_method, window=20):\n",
    "    for action_selection in action_selections:\n",
    "        start = time.time()\n",
    "\n",
    "        print(f\"Training with {action_selection.__name__}...\")\n",
    "        # Train the agent using the specified action selection method and learning method\n",
    "        rewards = learn(env, num_episodes, action_selection, learning_method)\n",
    "\n",
    "        # Display information about video recording folder\n",
    "        print(f\"{action_selection.__name__} Video in {folder_name}\")\n",
    "\n",
    "        # Calculate rolling mean of rewards for smoother visualization\n",
    "        average_reward = [np.nan]*(window - 1)\n",
    "        for x in range(len(rewards) - window + 1):\n",
    "            average_reward.append(np.mean(rewards[x:x+window]))\n",
    "\n",
    "        end  = time.time()\n",
    "        time_elapsed = end - start\n",
    "\n",
    "        print(f\"Time spent on {action_selection.__name__} is {time_elapsed} s with {learning_method.__name__} learning method\")\n",
    "\n",
    "        # Plot the rewards and rolling mean rewards\n",
    "        plt.plot(rewards, label=\"Reward\")\n",
    "        plt.plot(average_reward, label=\"Rolling mean reward\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "# Create the environment with video recording\n",
    "env, folder_name = create_environment(\"Taxi-v3\", True)\n",
    "\n",
    "# List of action selection methods\n",
    "action_selections = [EpsilonGreedy, UpperConfidenceBound, BoltzmannExploration, ThompsonSampling]\n",
    "\n",
    "# Call the function\n",
    "train_and_plot(env, 1001, action_selections, QNeural)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
